
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from tqdm import tqdm
import os
from datetime import datetime
import matplotlib.pyplot as plt

from src.data.mnist import get_mnist_loader_and_transform


# ==================== ç®€å•çš„åˆ†ç±»å™¨æ¶æ„ ====================

class WeakClassifier(nn.Module):

    def __init__(self, num_classes=10):
        super(WeakClassifier, self).__init__()
        
        # æ›´ç®€å•çš„å·ç§¯å±‚
        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)  # 16 é€šé“ï¼ˆvs 32+ï¼‰
        self.bn1 = nn.BatchNorm2d(16)
        
        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)  # 32 é€šé“ï¼ˆvs 64+ï¼‰
        self.bn2 = nn.BatchNorm2d(32)
        
        self.pool = nn.MaxPool2d(2, 2)
        
        # æ›´å¼ºçš„dropout
        self.dropout1 = nn.Dropout(0.4)  # æ›´é«˜çš„dropout
        self.dropout2 = nn.Dropout(0.5)
        
        # æ›´å°çš„å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(32 * 7 * 7, 64)  # æ›´å°çš„éšè—å±‚
        self.fc2 = nn.Linear(64, num_classes)
    
    def forward(self, x):
        # Block 1
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool(x)  # 28x28 -> 14x14
        x = self.dropout1(x)
        
        # Block 2
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.pool(x)  # 14x14 -> 7x7
        x = self.dropout1(x)
        
        # Classifier
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.dropout2(x)
        x = self.fc2(x)
        
        return x


class VeryWeakClassifier(nn.Module):
    """
    æ›´å¼±çš„åˆ†ç±»å™¨ï¼ˆå¦‚æœéœ€è¦æ›´ä½å‡†ç¡®ç‡ï¼‰
    ç›®æ ‡: 90-95% å‡†ç¡®ç‡
    """
    def __init__(self, num_classes=10):
        super(VeryWeakClassifier, self).__init__()
        
        # åªæœ‰ä¸€ä¸ªå·ç§¯å±‚
        self.conv1 = nn.Conv2d(1, 16, 5, padding=2)
        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(0.5)
        
        # ç®€å•çš„å…¨è¿æ¥
        self.fc1 = nn.Linear(16 * 7 * 7, 32)
        self.fc2 = nn.Linear(32, num_classes)
    
    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(x)  # 28x28 -> 14x14
        x = self.pool(x)  # 14x14 -> 7x7
        x = self.dropout(x)
        
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        
        return x


# ==================== è®­ç»ƒå’Œè¯„ä¼°å‡½æ•° ====================

def train_epoch(model, train_loader, criterion, optimizer, device, epoch):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    pbar = tqdm(train_loader, desc=f'Epoch {epoch} [Train]')
    for images, labels in pbar:
        images, labels = images.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        
        pbar.set_postfix({
            'loss': f'{running_loss/(pbar.n+1):.4f}',
            'acc': f'{100.*correct/total:.2f}%'
        })
    
    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100. * correct / total
    
    return epoch_loss, epoch_acc


def evaluate(model, val_loader, criterion, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    
    class_correct = [0] * 10
    class_total = [0] * 10
    
    with torch.no_grad():
        pbar = tqdm(val_loader, desc='Validating')
        for images, labels in pbar:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            for i in range(labels.size(0)):
                label = labels[i].item()
                class_correct[label] += (predicted[i] == labels[i]).item()
                class_total[label] += 1
            
            pbar.set_postfix({
                'loss': f'{running_loss/(pbar.n+1):.4f}',
                'acc': f'{100.*correct/total:.2f}%'
            })
    
    val_loss = running_loss / len(val_loader)
    val_acc = 100. * correct / total
    
    return val_loss, val_acc, class_correct, class_total


# ==================== ä¸»å‡½æ•° ====================

def main():
    # ==================== é…ç½®å‚æ•° ====================
    EPOCHS = 1  # æ›´å°‘çš„epochï¼ˆæ•…æ„ä¸å……åˆ†è®­ç»ƒï¼‰
    LEARNING_RATE = 0.001
    WEIGHT_DECAY = 5e-4  # æ›´å¼ºçš„æƒé‡è¡°å‡
    BATCH_SIZE = 128
    
    MODEL_TYPE = "very_weak"  # "weak" or "very_weak"
    PATH_TO_SAVE_MODEL = "weak_classifier.pth"
    
    # åˆ›å»ºå¿…è¦çš„æ–‡ä»¶å¤¹
    os.makedirs("checkpoints", exist_ok=True)
    os.makedirs("projects/diffusion/stable-diffusion-from-scratch/loss", exist_ok=True)
    
    # è®¾ç½®è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    print(f"{'='*70}")
    print(f"Training Weak MNIST Classifier for Guidance")
    print(f"{'='*70}")
    print(f"Device: {device}")
    print(f"Model Type: {MODEL_TYPE}")
    print(f"Epochs: {EPOCHS}")
    print(f"Target Accuracy: 95-98%")
    print(f"{'='*70}\n")
    
    # ==================== åŠ è½½æ•°æ® ====================
    print("Loading MNIST dataset...")
    data = get_mnist_loader_and_transform()
    
    print(f"âœ“ Train samples: {len(data.train_dataset)}")
    print(f"âœ“ Val samples: {len(data.val_dataset)}\n")
    
    # ==================== åˆ›å»ºæ¨¡å‹ ====================
    print(f"Creating {MODEL_TYPE} classifier...")
    if MODEL_TYPE == "weak":
        model = WeakClassifier(num_classes=10).to(device)
    elif MODEL_TYPE == "very_weak":
        model = VeryWeakClassifier(num_classes=10).to(device)
    else:
        raise ValueError(f"Unknown model type: {MODEL_TYPE}")
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"âœ“ Model parameters: {total_params:,}")
    print(f"  (æ¯”è¾ƒ: ResNet-18 æœ‰ ~11M å‚æ•°)\n")
    
    # ==================== å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ ====================
    # ä½¿ç”¨Label Smoothingè¿›ä¸€æ­¥é™ä½è¿‡æ‹Ÿåˆ
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
    
    # ä¸ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œä¿æŒç®€å•
    
    # ==================== è®­ç»ƒå¾ªç¯ ====================
    print(f"{'='*70}")
    print("Starting Training")
    print(f"{'='*70}\n")
    
    best_acc = 0.0
    train_losses = []
    val_losses = []
    train_accs = []
    val_accs = []
    
    # æ—©åœæœºåˆ¶ï¼šè¾¾åˆ°ç›®æ ‡å‡†ç¡®ç‡å°±åœæ­¢
    target_acc_min = 95.0
    target_acc_max = 98.0
    
    for epoch in range(1, EPOCHS + 1):
        print(f"\n{'='*70}")
        print(f"Epoch {epoch}/{EPOCHS}")
        print(f"{'='*70}")
        
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(
            model, data.train_loader, criterion, optimizer, device, epoch
        )
        
        # éªŒè¯
        val_loss, val_acc, class_correct, class_total = evaluate(
            model, data.val_loader, criterion, device
        )
        
        # è®°å½•å†å²
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)
        
        # æ‰“å°ç»“æœ
        print(f"\n{'â”€'*70}")
        print(f"Epoch {epoch} Summary:")
        print(f"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
        print(f"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%")
        
        # æ‰“å°æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        print(f"\n  Per-class Accuracy:")
        for i in range(10):
            class_acc = 100. * class_correct[i] / class_total[i]
            bar = 'â–ˆ' * int(class_acc / 2)
            print(f"    Digit {i}: {class_acc:5.2f}% {bar}")
        print(f"{'â”€'*70}")
        
        # ä¿å­˜æ¨¡å‹ï¼ˆæ¯ä¸ªepochéƒ½ä¿å­˜ï¼Œå› ä¸ºæˆ‘ä»¬è¦çš„æ˜¯"å¼±"åˆ†ç±»å™¨ï¼‰
        if val_acc > best_acc:
            best_acc = val_acc
        
        save_path = os.path.join("checkpoints", PATH_TO_SAVE_MODEL)
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_acc': val_acc,
            'val_loss': val_loss,
            'train_acc': train_acc,
            'train_loss': train_loss,
            'model_type': MODEL_TYPE,
        }, save_path)
        
        print(f"\nâœ“ Saved model to {save_path}")
        print(f"  Current Val Accuracy: {val_acc:.2f}%")
        
        # æ—©åœï¼šå¦‚æœè¾¾åˆ°ç›®æ ‡å‡†ç¡®ç‡èŒƒå›´ï¼Œå°±åœæ­¢
        if target_acc_min <= val_acc <= target_acc_max:
            print(f"\nğŸ¯ è¾¾åˆ°ç›®æ ‡å‡†ç¡®ç‡èŒƒå›´ ({target_acc_min}% - {target_acc_max}%)!")
            print(f"   å½“å‰å‡†ç¡®ç‡: {val_acc:.2f}%")
            print(f"   åœæ­¢è®­ç»ƒï¼Œè¿™ä¸ªæ¨¡å‹é€‚åˆç”¨äºå¼•å¯¼ç”Ÿæˆã€‚")
            break
        elif val_acc > target_acc_max:
            print(f"\nâš  å‡†ç¡®ç‡è¶…è¿‡ç›®æ ‡ä¸Šé™ ({val_acc:.2f}% > {target_acc_max}%)")
            print(f"   è¿™ä¸ªæ¨¡å‹å¯èƒ½å¤ªå¼ºäº†ï¼Œå»ºè®®ä½¿ç”¨ 'very_weak' æ¨¡å‹ç±»å‹")
    
    # ==================== ä¿å­˜è®­ç»ƒæ›²çº¿ ====================
    print(f"\n{'='*70}")
    print("Saving training curves...")
    print(f"{'='*70}")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label="Train Loss", linewidth=2)
    plt.plot(val_losses, label="Val Loss", linewidth=2)
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title(f"Loss Curves (Final: Val={val_losses[-1]:.4f})")
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(train_accs, label="Train Accuracy", linewidth=2)
    plt.plot(val_accs, label="Val Accuracy", linewidth=2)
    plt.axhline(y=target_acc_min, color='g', linestyle='--', alpha=0.5, label='Target Min')
    plt.axhline(y=target_acc_max, color='r', linestyle='--', alpha=0.5, label='Target Max')
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy (%)")
    plt.title(f"Accuracy Curves (Final: Val={val_accs[-1]:.2f}%)")
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    img_path = os.path.join("projects/diffusion/stable-diffusion-from-scratch/loss", 
                            f"weak_classifier_curves_{timestamp}.png")
    plt.savefig(img_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… Training curves saved to: {img_path}")
    
    # ==================== è®­ç»ƒå®Œæˆ ====================
    print(f"\n{'='*70}")
    print("Training Completed!")
    print(f"{'='*70}")
    print(f"Final Val Accuracy: {val_accs[-1]:.2f}%")
    print(f"Model saved to: {os.path.join('checkpoints', PATH_TO_SAVE_MODEL)}")
    
    if target_acc_min <= val_accs[-1] <= target_acc_max:
        print(f"âœ… æˆåŠŸï¼å‡†ç¡®ç‡åœ¨ç›®æ ‡èŒƒå›´å†…ï¼Œé€‚åˆç”¨äºå¼•å¯¼ç”Ÿæˆã€‚")
    elif val_accs[-1] < target_acc_min:
        print(f"âš  å‡†ç¡®ç‡åä½ï¼Œå¯èƒ½éœ€è¦è®­ç»ƒæ›´å¤šepochæˆ–ä½¿ç”¨'weak'æ¨¡å‹")
    else:
        print(f"âš  å‡†ç¡®ç‡åé«˜ï¼Œå»ºè®®ä½¿ç”¨'very_weak'æ¨¡å‹æˆ–å‡å°‘è®­ç»ƒepoch")
    
    print(f"{'='*70}\n")


if __name__ == '__main__':
    main()